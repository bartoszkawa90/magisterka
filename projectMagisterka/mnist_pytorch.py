# -*- coding: utf-8 -*-
"""mnist_pytorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jd3ZBWCdIGpZ6THfyWMzcRB8Qry3Gi27
"""

import torch as t
from torch import nn
import tensorflow as tf
from torchsummary import summary
import numpy as np
from torch import optim
from random import randint
from functools import reduce

class Net(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(Net, self).__init__()
        self.net = nn.Sequential(
            nn.Flatten(),
            nn.Linear(np.prod(input_size), hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, output_size),
            nn.Softmax(dim=1)
        )

    def forward(self, x):
        return self.net(x)

# test and training data
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255

# choose network, objective function and optimizer
net = Net(input_size=(28, 28), hidden_size=128, output_size=10)
objective = nn.CrossEntropyLoss()
optimizer = optim.Adam(params=net.parameters(), lr=0.001)
# summary(net, (1, 28, 28))

iter = 0
for x_tr, y_tr in zip(x_train, y_train):
  optimizer.zero_grad()
  # print(x_tr.shape, x_tr)
  y_got = net(t.tensor(x_tr).unsqueeze(0))
  print(f'got tensor {y_got}')
  print(f'got max {t.argmax(y_got)}, expected {y_tr}')
  loss = objective(y_got, t.tensor([y_tr]).long())
  loss.backward()
  optimizer.step()
  print(f'loss: {loss}, iter {iter} \n')
  iter += 1

# try network
score, iter = 0, 0
for i in range(randint(2, 200)):
  test = net(t.tensor(x_test[i]).unsqueeze(0))
  print(f'got tensor {test}')
  if t.argmax(test) == y_test[i]:
    score += 1
  print(f'got max {t.argmax(test)}, expected {y_test[i]}')
  iter += 1
print(score/iter)